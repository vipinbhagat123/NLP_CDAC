{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMOrsgcMFSmt4DoWx1makEp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vipinbhagat123/NLP_CDAC/blob/main/udemy_stemmer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üî† Most Common Stemmers Used in Industry\n",
        "\n",
        "Stemming is a text normalization technique used to reduce words to their base or root form. In industry, stemming is still used in search engines and simple NLP pipelines, although **lemmatization** is more common today in production systems.\n",
        "\n",
        "---\n",
        "\n",
        "## üîù Top Stemmers in Use\n",
        "\n",
        "### 1. ‚úÖ Porter Stemmer\n",
        "- **Most famous**, created by Martin Porter (1980).\n",
        "- Example: `\"running\"` ‚Üí `\"run\"`, `\"flies\"` ‚Üí `\"fli\"`\n",
        "- **Pros**: Fast, simple, widely supported.\n",
        "- **Cons**: Often too aggressive, not context-aware.\n",
        "- **Used in**: Lucene, classic search engines.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. ‚úÖ Snowball Stemmer (Porter2)\n",
        "- An improved version of the Porter stemmer.\n",
        "- Supports **multiple languages** (English, Spanish, etc.).\n",
        "- Example: `\"organization\"` ‚Üí `\"organ\"`\n",
        "- **Pros**: Cleaner rules, more consistent.\n",
        "- **Used in**: Elasticsearch, Solr, NLTK.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. ‚ö†Ô∏è Lancaster Stemmer\n",
        "- Developed at Lancaster University.\n",
        "- Very aggressive stemming.\n",
        "- Example: `\"maximum\"` ‚Üí `\"maxim\"`, `\"running\"` ‚Üí `\"run\"`\n",
        "- **Pros**: Strong normalization.\n",
        "- **Cons**: Over-stemming and distortion.\n",
        "- **Rare in production** due to harsh truncation.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. üß© Custom/Regex-Based Stemmers\n",
        "- Designed for domain-specific language (e.g., finance, science).\n",
        "- Example: `\"geolocation\"`, `\"geolocate\"` ‚Üí `\"geo\"`\n",
        "- **Pros**: Tuned to application needs.\n",
        "- **Cons**: Requires maintenance, not generalizable.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. üï∞Ô∏è Lovins Stemmer\n",
        "- One of the earliest stemmers (1968).\n",
        "- Removes longest matching suffix.\n",
        "- Example: `\"connections\"` ‚Üí `\"connect\"`\n",
        "- **Rarely used today**, but historically notable.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öñÔ∏è Stemming vs Lemmatization\n",
        "\n",
        "| Feature            | Stemming                | Lemmatization           |\n",
        "|--------------------|--------------------------|---------------------------|\n",
        "| Approach           | Rule-based suffix stripping | Dictionary + POS-based |\n",
        "| Output             | `\"running\"` ‚Üí `\"run\"`<br>`\"flies\"` ‚Üí `\"fli\"` | `\"ran\"` ‚Üí `\"run\"` |\n",
        "| Speed              | Fast                    | Slower but accurate     |\n",
        "| Context-awareness  | ‚ùå                      | ‚úÖ                      |\n",
        "| Use Case           | IR/search, basic NLP    | ML/NLP pipelines, chatbots |\n",
        "\n",
        "---\n",
        "\n",
        "## üíº Industrial Use Cases\n",
        "\n",
        "- **Search Engines (e.g., Elasticsearch, Solr)**: Porter or Snowball stemmers.\n",
        "- **Machine Learning Pipelines (e.g., scikit-learn)**: Sometimes stemmers, more often lemmatization.\n",
        "- **Deep Learning Models (e.g., BERT)**: Typically use **raw text**, no stemming.\n",
        "\n",
        "---\n",
        "\n",
        "> üîé Tip: Use **lemmatization** if you care about word meaning and grammar. Use **stemming** if you need speed and simplicity.\n",
        "\n"
      ],
      "metadata": {
        "id": "6B2b0We0_wqt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "RLC5R14p_Ybr",
        "outputId": "fc6766cc-bda6-4eae-f1ca-827ecdd11e56"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Stemming is the process of reducing a word to its word stem that\\naffixes to suffixes and prefixes or to the roots of words known as a \\nlemma. Stemming is important in natural language understanding (NLU) and natural \\nlanguage processing (NLP).'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "\n",
        "# Stemming\n",
        "\n",
        "\"\"\"Stemming is the process of reducing a word to its word stem that\n",
        "affixes to suffixes and prefixes or to the roots of words known as a\n",
        "lemma. Stemming is important in natural language understanding (NLU) and natural\n",
        "language processing (NLP).\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification problems\n",
        "## comments of produt is a positvie review or negative review\n",
        "## Reviews --> eating, eat, eaten [going, gone, goes] --->go\n",
        "words=[\"eating\",\"eats\",\"eaten\",\"writing\",\"writes\",\"programming\",\"programs\",\"history\",\"finally\",\"finalized\"]\n"
      ],
      "metadata": {
        "id": "jfKYa0kjAPH6"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PorterStemmer"
      ],
      "metadata": {
        "id": "jtbLyUjfAowM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemming = PorterStemmer()\n"
      ],
      "metadata": {
        "id": "QQFQWNoNAoCl"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "  print(word+\"--->\"+stemming.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLcy4YIQAz8X",
        "outputId": "5d5ed6f1-6c28-4725-ed6e-9679c5349a11"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating--->eat\n",
            "eats--->eat\n",
            "eaten--->eaten\n",
            "writing--->write\n",
            "writes--->write\n",
            "programming--->program\n",
            "programs--->program\n",
            "history--->histori\n",
            "finally--->final\n",
            "finalized--->final\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemming.stem('congratulations')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "B2H6XQdYA9uL",
        "outputId": "c26e5059-102b-4694-92db-a1a9e9efb94a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'congratul'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemming.stem('sitting')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "fp_wKX8xBDgO",
        "outputId": "aa29ad79-6770-42f2-c34e-13f485a19174"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'sit'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RegexpStemmer class\n"
      ],
      "metadata": {
        "id": "BbepkYE-BOcc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RegexpStemmer class\n",
        "\n",
        "NLTK has RegexpStemmer class with the help of which we can easily implement Regular Expression Stemmer algorithms. It basically takes a single regular expression and removes any prefix or suffix that matches the expression. Let us see an example"
      ],
      "metadata": {
        "id": "tpmZ7XMYBRps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import RegexpStemmer\n",
        "reg_stemmer=RegexpStemmer('ing$|s$|e$|able$', min=4)"
      ],
      "metadata": {
        "id": "gUdYnK_z9CwM"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg_stemmer.stem('eating')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "wvIG6cQYBmPM",
        "outputId": "9da1dafa-e341-4ba5-ce1e-54efb8046b0c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'eat'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reg_stemmer.stem('ingeating')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "wsfF1e6QBxG5",
        "outputId": "6a3c0b78-b10b-421c-f880-b4ca54d0fbe6"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ingeat'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Snowball Stemmer\n",
        "\n",
        "It is a stemming algorithm which is also known as the Porter2 stemming algorithm as it is a better version of the Porter Stemmer since some issues of it were fixed in this stemmer."
      ],
      "metadata": {
        "id": "1lrn6SjJB3eL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer"
      ],
      "metadata": {
        "id": "sWAnZjg1B0rs"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "snowballsstemmer = SnowballStemmer('english')\n"
      ],
      "metadata": {
        "id": "ZNZW08sQB9NP"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "  print(word+\"--->\" + snowballsstemmer.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zY1j1SECHLK",
        "outputId": "e2b8f48f-2c7a-4c55-a4d3-4306aed9b1a9"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating--->eat\n",
            "eats--->eat\n",
            "eaten--->eaten\n",
            "writing--->write\n",
            "writes--->write\n",
            "programming--->program\n",
            "programs--->program\n",
            "history--->histori\n",
            "finally--->final\n",
            "finalized--->final\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemming.stem(\"fairly\"),stemming.stem(\"sportingly\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18WXx5o4BHBL",
        "outputId": "573f3af5-5291-43b2-cead-c84589e55ec7"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('fairli', 'sportingli')"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "snowballsstemmer.stem(\"fairly\"),snowballsstemmer.stem(\"sportingly\")\n"
      ],
      "metadata": {
        "id": "LWqADUtsDbtp",
        "outputId": "9f6bcdca-c52c-4cf0-87b9-926b51246516",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('fair', 'sport')"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "snowballsstemmer.stem('goes')\n"
      ],
      "metadata": {
        "id": "HmlRySzhDeep",
        "outputId": "1d7ddc3b-3150-44b0-c6dc-eac894074d4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'goe'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemming.stem('goes')\n"
      ],
      "metadata": {
        "id": "HsT_ptqWDgW6",
        "outputId": "04801cad-8176-4ff2-c518-03cffeb60005",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'goe'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dwiVo3PRDh6a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}