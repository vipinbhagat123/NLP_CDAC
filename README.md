# Natural Language Processing (NLP) - PG-DAI February 2025

This repository contains the suggested teaching guidelines for the Natural Language Processing (NLP) course, part of the PG-DAI program at ACTS, Pune, scheduled for February 2025.

---

## Course Overview

* **Total Hours:** 26 Hours Theory (T) + 44 Hours Lab (L)
* **Course Objective:** To provide a comprehensive understanding of Natural Language Processing, from foundational linguistic concepts to advanced deep learning models and their practical applications.

---

## Session Breakdown

### Session 1: Language in Cognitive Science (2 Hrs Theory)

* Definitions of language
* Language as a rule-governed dynamic system
* Knowledge of language: Innateness of Grammar
* Language as a biological, social, and psychological phenomenon
* Modes of language: spoken and written
* Language system as expression and content
* Language and symbolic systems: Artificial Language (Logical language/programming Language) vs. Natural Language
* Linguistics as a scientific study

### Session 2: Language Analysis and Computational Linguistics (2 Hrs Theory)

* **Language Analysis:**
    * Paradigmatic and Syntagmatic relationship
    * Form, Function, and Meaning in Language Analysis
    * **Levels of Linguistic Analysis:** Phonetics, Phonology, Morphology, Syntax, Semantics, Discourse, Pragmatics, Lexicology
* **Artificial Intelligence (AI) and its sub-disciplines:**
    * Natural Language Understanding (NLU)
    * Natural Language Generation (NLG)
    * Natural Language Interaction (NLI)
* **Assignment:** Difference between Semantics and Pragmatics

---

### Session 3: Shallow Parsing and Tools for NLP (2 Hrs Theory + 4 Hrs Lab)

* **Lecture:**
    * Morphological Analysis
    * Tokenization & PoS Tagging
    * Chunking & Multi-word expression (MWE)
    * Named-Entity Recognition
    * Lemmatizer & Stemming
    * Morphological Synthesis
    * Word Sense Disambiguation
    * Universal Networking Language
* **Assignment:** Apply tokenization, stemming, lemmatization, and POS tagging for the following data: [Disneyland Reviews on Kaggle](https://www.kaggle.com/arushchillar/disneyland-reviews)

---

### Session 4: Deep Parsing and Tools for NLP (2 Hrs Theory + 4 Hrs Lab)

* **Lecture:**
    * Syntactic Parsing Techniques and algorithms
    * Semantic Parsing
    * Information Extraction
    * Automatic Summarization
    * Anaphora Resolution, Pragmatics, and Discourse analysis
    * Ontology & Semantic Web

---

### Session 5: Statistical Approaches (2 Hrs Theory + 4 Hrs Lab)

* **Lecture:**
    * Probability Theory & Models
    * Discrete Time Models
    * Markov Models, Entropy Models
    * Statistical Parsing
    * Text Categorization / Classification and Clustering
    * Text Classification Using Support Vector Machine (SVM)
    * Centroid based Classification

---

### Session 6: NLP with Machine Learning and Deep Learning (2 Hrs Theory + 4 Hrs Lab)

* **Lecture:**
    * Machine Learning & Deep Learning for NLP
    * ML vs. DL along with respective algorithms names
    * Linear Algebra Review
    * ANN, RNN, CNN, DNN Long Short-Term Memory (LSTM)
    * Gated Feedback Recurrent Neural Networks
* **Assignment:** Difference between ANN, RNN, and CNN

---

### Session 7: Study of Important ML/DL Algorithms & Case Studies (2 Hrs Theory + 4 Hrs Lab)

* **Lecture:**
    * Pre-processing
    * Need of Pre-processing Data
    * Introduction to NLTK, spaCy
    * Using Python Scripts

---

### Session 8: Advanced Neural Networks for NLP (2 Hrs Theory + 4 Hrs Lab)

* **Lecture:**
    * Introduction to gates, GRU, and LSTM
    * How LSTM and GRU resolve exploding and vanishing gradient problems
* **Lab Assignment:**
    * Implement LSTM and GRU in Python
    * Sequence Modeling Lab
    * Using TensorFlow and Python for music generation using deep learning

---

### Session 9: Sequence Modeling - Word Representation & Sentiment Classification (2 Hrs Theory + 4 Hrs Lab)

* **Lecture:**
    * Sequence Modeling - Word representation
    * Word embedding matrix
    * Learning words and embeddings
    * Sentiment classifications

---

### Session 10: Word2Vec, Seq2Seq Models & Transformers (2 Hrs Theory + 4 Hrs Lab)

* **Lecture:**
    * **Word2Vec models:** Skip-gram, CBOW, GloVe, One-hot Encoding
    * **Sequence-to-sequence models (Seq2Seq):**
        * GloVe: Global Vectors for Word representation
        * Brief on Sequence-to-Sequence Models
        * RNN based Sequence-to-Sequence Model
        * Challenges
    * **Transformer in NLP:**
        * Understanding the Model Architecture
        * Getting Hang of Self-Attention
        * Calculation of Self-Attention
        * Limitations of the Transformer
    * **Transformer-XL:**
        * Using Transformer for Language Modeling
        * Using Transformer-XL for Language Modeling

---

### Session 11: BERT & NLP Model Deployment (2 Hrs Theory + 4 Hrs Lab)

* **Lecture:**
    * Bert in NLP
    * Model Architecture
    * BERT Pre-Training Tasks
    * NLP Model Deployment Techniques using Flask

---

### Session 12 & 13: Speech Processing & NLP Applications (4 Hrs Theory + 8 Hrs Lab)

* **Lecture (Speech Processing):**
    * Articulatory Phonetics
    * Speech Sounds and Phonetic Transcription
    * Acoustic Phonetics
    * Phonology
    * Computational Phonology
    * Digital Signal Processing Techniques
    * Automatic Speech Recognition (ASR)
    * Speech Recognition Approaches
    * Text to Speech (TTS) system
    * Speech Synthesis Approaches
    * Language Models
* **Lab Session: NLP Applications:**
    * Lexicon, Dictionaries, Thesaurus
    * Transliteration, Spell Checker, Grammar Checker, Domain identification, Language identification
    * Auto suggest/Auto complete, Gender Prediction
    * Machine Translation (including Neural Machine Translation)
    * Information extraction and Retrieval
    * Question answering & dialogue agents
    * Speech Technologies
    * OCR, Hand Writing Recognition
    * Indian Language Script Technology
    * Chatbots
    * Robotics (NLP aspects)
